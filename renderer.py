import jax
from absl import app, flags

import jax.numpy as jnp
import optax
import flax
from flax.training import checkpoints, train_state
from flax.metrics import tensorboard
import tensorflow as tf
import numpy as np
import functools
import yaml
import cv2
import os
from dataclasses import dataclass

from nerf import (
    get_model,
    get_nerf,
)
from nerf_config import get_config, NerfConfig
from datasets import dataset_factory, trans, rot_z, rot_y, rot_x

FLAGS = flags.FLAGS
flags.DEFINE_string("config_path", None, "config file path")
flags.DEFINE_string("render_output_folder", None, "where to output the rendered images")
flags.DEFINE_bool(
    "display_render", False, "whether to show the rendered images using cv2.imshow"
)
flags.mark_flag_as_required("config_path")


def get_nerf_eval_func(config: NerfConfig):
    """Return NeRF function for evaluation"""
    return get_nerf(
        near=config.near,
        far=config.far,
        L_position=config.L_position,
        L_direction=config.L_direction,
        num_samples_coarse=config.num_samples_coarse,
        num_samples_fine=config.num_samples_fine,
        use_hvs=config.use_hvs,
        use_direction=True,
        use_random_noise=False,
    )


def render_step(nerf_func, state, val_data, eval_batch_size):
    """
    Render step, takes in an entire image and returns the predicted image

    Inputs:
        nerf_func: a function performs the nerf algorithm
        state: replicated train state
        val_data: origins and directions of rays each with [H, W, 3]
        eval_batch_size: batch size for evaluation
    Outputs:
        pred_img: predicted images [H, W, 3]
        depth_img: predicted images [H, W, 3]

    """

    # for eval key stays the same
    key = jax.random.PRNGKey(0)

    eval_origins = val_data[0]
    eval_directions = val_data[1]

    origins_flattened = eval_origins.reshape(-1, 3)
    directions_flattened = eval_directions.reshape(-1, 3)

    pred_img_parts = []
    pred_depth_parts = []
    for i in range(0, origins_flattened.shape[0], eval_batch_size):
        origin = origins_flattened[i : i + eval_batch_size]
        direction = directions_flattened[i : i + eval_batch_size]

        (rendered, rendered_hvs), weights, ts = nerf_func(
            params=state.params,
            key=key,
            origins=origin,
            directions=direction,
        )

        depth_hvs = jnp.sum(weights * ts, -1)

        pred_img_parts.append(rendered_hvs)
        pred_depth_parts.append(depth_hvs)

    # Reshape image
    pred_img = jnp.concatenate(pred_img_parts, axis=0)
    pred_img = pred_img.reshape(eval_origins.shape)

    # expand dims tf.ssims expects [B, H, W, C]
    pred_img = jnp.expand_dims(pred_img, axis=0)

    # Reshape depth image
    pred_depth = jnp.concatenate(pred_depth_parts, axis=0)
    pred_depth = pred_depth.reshape(eval_origins.shape[:-1])
    pred_depth = jnp.expand_dims(pred_depth, axis=[0, -1])

    return pred_img, pred_depth


def save_img(img, file_path, multiplier=255.0):
    img = np.asarray(img * multiplier).astype(np.uint8)
    img = np.squeeze(img)

    cv2.imwrite(file_path, img)

    return img


def render_nerf(config: NerfConfig, render_output_folder: str, display_render: bool):
    """
    Render NeRF model based the poses generated by `custom_path_generator`
    function below.
    """
    config.scale = 0.15
    dataset = dataset_factory(config)

    # Set near and far for LLFF datasets
    if config.dataset_type == "llff":
        # set near and far to calculated values
        config.near = dataset["train"].near
        config.far = dataset["train"].far

    # Load model and state
    model, _ = get_model(config.L_position, config.L_direction)
    state_dict = checkpoints.restore_checkpoint(config.load_ckpt_dir, target=None)
    eval_state = train_state.TrainState.create(
        apply_fn=model, params=state_dict["params"], tx=optax.adam(1)
    )

    # Create nerf function
    eval_nerf_func = get_nerf_eval_func(config)
    jit_eval_nerf_func = jax.jit(functools.partial(eval_nerf_func, model_func=model))

    def custom_path_generator(poses):
        """
        Generate rendering poses from training set poses.

        Args:
            poses (numpy.ndarray): An array of camera poses.

        Returns:
            A list of rendering poses.

        """
        # This  creates a movement in the xy-axis and a slight rotation
        # about the z-axis.
        c2w = poses.mean(0)

        if c2w.shape[0] == 3:
            c2w = np.vstack([c2w, np.array([0, 0, 0, 1])])

        x = -0.0
        y = -0.0
        z = 1

        dx = 0.1
        dy = 0.1
        dz = -0.1
        angle = -0.0
        d_angle = -0.05

        render_poses = []
        for idx in range(1, 50):
            sin_angle = np.sin(idx / 3.0) / 5.0

            if idx < 17:
                print(f"angle_exp : {angle}")
                transformed_c2w = trans(y=y, x=x, z=z) @ rot_y(sin_angle) @ c2w

                # x += dx
                y += dy
                z += dz
                angle += d_angle
            else:
                if idx == 17:
                    angle = 0
                print(f"angle_exp : {angle}")
                transformed_c2w = trans(y=y, x=x, z=z) @ rot_x(angle) @ c2w

                angle += d_angle

            render_poses.append(transformed_c2w)

        return render_poses

    # Set rendering poses for dataset
    dataset["render"].render_poses = custom_path_generator(dataset["render"].poses)

    # Render images and save to file
    for idx, data in enumerate(dataset["render"]):
        print(f"Rendering image {idx}...")
        pred_img, pred_depth = render_step(
            jit_eval_nerf_func, eval_state, data, config.batch_size
        )

        output_folder = f"rendered_imgs_{config.dataset_name}"
        output_path = f"{output_folder}/{str(idx).zfill(5)}.png"
        os.makedirs(output_folder, exist_ok=True)
        uint8_img = save_img(pred_img, output_path)
        if display_render:
            cv2.imshow(f"{idx}", uint8_img)
            cv2.waitKey(0)


def nerf_to_mesh(config: NerfConfig):
    """
    This function converts a NeRF model into a 3D mesh
    using Open3D.

    """
    raise NotImplementedError


def main(argv):
    # Remove GPUs for tf
    tf.config.experimental.set_visible_devices([], "GPU")

    # Load config from yaml file
    config = get_config(FLAGS.config_path)

    # Render NeRF
    render_nerf(
        config,
        render_output_folder=FLAGS.render_output_folder,
        display_render=FLAGS.display_render,
    )


if __name__ == "__main__":
    app.run(main)
